{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Tool Calling 101\n",
    "\n",
    "\n",
    "\n",
    "Tool calling is one of the most important aspects when building with agents. In this part of the tutorial I'll show you how to implement simple tool/function calling with DeepSeek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we setup credentials: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import jupyter_black\n",
    "from openai import OpenAI\n",
    "from typing import Any\n",
    "import inspect\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "load_dotenv(dotenv_path=\"api_keys.env\")\n",
    "\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "BASE_URL = \"https://api.deepseek.com\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a function to call our llm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_messages(messages, model=MODEL, tools=None):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages, tools=tools, temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use it to call our LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To check the current weather in Lisbon, you can use reliable weather services like **Weather.com (The Weather Channel), AccuWeather, or Meteoblue**. Here’s a general idea of Lisbon’s climate:  \n",
      "\n",
      "- **Summer (June–September):** Warm and sunny, with temperatures around **25–30°C (77–86°F)**, sometimes reaching **35°C (95°F)** in heatwaves.  \n",
      "- **Autumn (October–November):** Mild, with occasional rain, temperatures **15–23°C (59–73°F)**.  \n",
      "- **Winter (December–February):** Cool and rainy, averaging **8–15°C (46–59°F)**, rarely below freezing.  \n",
      "- **Spring (March–May):** Pleasant, gradually warming, with temperatures **12–22°C (54–72°F)**.  \n",
      "\n",
      "For **real-time updates**, I recommend checking:  \n",
      "- [Weather.com (Lisbon)](https://weather.com)  \n",
      "- [AccuWeather (Lisbon)](https://www.accuweather.com)  \n",
      "- [IPMA (Portuguese Institute)](https://www.ipma.pt)  \n",
      "\n",
      "Would you like a forecast for a specific date? 😊\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    send_messages(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"How is the weather in Lisbon?\"}],\n",
    "    ).content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function to fetch the weather: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherRequest(BaseModel):\n",
    "    city: str\n",
    "\n",
    "\n",
    "def get_weather(request: WeatherRequest) -> str:\n",
    "    return f\"The weather in {request.city} is sunny\"\n",
    "\n",
    "\n",
    "def to_openai_tool(func: callable) -> dict[str, Any]:\n",
    "    sig = inspect.signature(func)\n",
    "    if sig.parameters:\n",
    "        param = next(iter(sig.parameters.values()))\n",
    "        param_type = param.annotation\n",
    "        schema = param_type.model_json_schema()\n",
    "    else:\n",
    "        param_type = None\n",
    "        schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (param_type.__doc__ if param_type else func.__doc__ or \"\"),\n",
    "            \"parameters\": schema,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we send that tool as well to DeepSeek: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message:  How is the weather in Lisbon?\n",
      "Tool call:  ChatCompletionMessageToolCall(id='call_0_5ced15ad-23f6-4a40-8aff-684bf5027f5b', function=Function(arguments='{\"city\":\"Lisbon\"}', name='get_weather'), type='function', index=0)\n"
     ]
    }
   ],
   "source": [
    "tools = [to_openai_tool(get_weather)]\n",
    "messages = [{\"role\": \"user\", \"content\": \"How is the weather in Lisbon?\"}]\n",
    "\n",
    "message = send_messages(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "messages.append(message)\n",
    "\n",
    "print(\"User message: \", messages[0][\"content\"])\n",
    "\n",
    "tool = message.tool_calls[0]\n",
    "print(\"Tool call: \", tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the result for that tool: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather\n",
      "{\"city\":\"Lisbon\"}\n",
      "The weather in Lisbon is sunny\n",
      "call_0_5ced15ad-23f6-4a40-8aff-684bf5027f5b\n"
     ]
    }
   ],
   "source": [
    "tool_name = tool.function.name\n",
    "tool_args = tool.function.arguments\n",
    "tool_result = globals()[tool_name](WeatherRequest.model_validate_json(tool_args))\n",
    "tool_call_id = tool.id\n",
    "\n",
    "print(tool_name)\n",
    "print(tool_args)\n",
    "print(tool_result)\n",
    "print(tool_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we send that result back to our LLM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool.id,\n",
    "        \"content\": tool_result,\n",
    "    }\n",
    ")\n",
    "message = send_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Lisbon is currently sunny. Enjoy the sunshine! ☀️\n"
     ]
    }
   ],
   "source": [
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise**: \n",
    "- Create a tool call to fetch the user's name and age and make the model use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_messages(messages, model=MODEL, tools=None):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages, tools=tools, temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message\n",
    "\n",
    "\n",
    "def get_user_info() -> str:\n",
    "    # 1. Implement this function. What information do we need about the user?\n",
    "    # Tip: Why not use the input function?\n",
    "    return \"I don't know\"\n",
    "\n",
    "\n",
    "tools = [to_openai_tool(get_user_info)]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is my name and age?\"}]\n",
    "message = send_messages(messages, tools=tools)\n",
    "print(f\"User message: {messages[0]['content']}\")\n",
    "\n",
    "tool = message.tool_calls[0]\n",
    "tool_args = tool.function.arguments\n",
    "tool_id = tool.id\n",
    "tool_name = tool.function.name\n",
    "messages.append(message)\n",
    "\n",
    "tool_result = globals()[tool_name]()\n",
    "\n",
    "messages.append({\"role\": \"tool\", \"tool_call_id\": tool.id, \"content\": tool_result})\n",
    "message = send_messages(messages)\n",
    "print(f\"Model>\\t {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that you know about tool calling! It's time to learn about agents vs. workflows!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
