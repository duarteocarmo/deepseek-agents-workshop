{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Agents vs. Workflows\n",
    "\n",
    "## Intro: What is an Agent? \n",
    "\n",
    "- Agents are systems that can \"_independently accomplish a task on our behalf_\"\n",
    "- Agents are composed of: \n",
    "    - **Task**: The goal that the agent wishes to accomplish for the user \n",
    "    - **Environment**: The space where the agent operates in, in order to accomplish the task\n",
    "    - **Tools**: The set of actions that the agent can take within the environment in order to accomplish the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents vs workflows\n",
    "\n",
    "- We have a clear idea of the steps the agent needs to accomplish: **use workflows**\n",
    "- We don't have a clear idea of the steps the agents needs to accomplish: **use agents**\n",
    "\n",
    "\n",
    "Cheatcheet: \n",
    "\n",
    "| Aspect           | Workflows                                         | Agents                                                      |\n",
    "|------------------|---------------------------------------------------|-------------------------------------------------------------|\n",
    "| Structure        | Predefined and sequential                         | Dynamic and autonomous                                      |\n",
    "| Flexibility      | Best for predictable, repeatable tasks            | Suited for complex, open-ended problems                     |\n",
    "| Control          | Driven by developer-written code paths            | LLM decides actions based on current context                |\n",
    "| Tool Usage       | Fixed order or recipe                             | Chosen and sequenced by the agent as needed                 |\n",
    "| Predictability   | Highly predictable                                | Variable; can adapt or retry based on feedback              |\n",
    "| Main Use Cases   | Well-defined automations, document flows          | Coding agents, interactive assistants, task planners        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow pattern: Evaluator optimizer\n",
    "\n",
    "![./assets/evaluator_optimizer.png](./assets/evaluator-optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup some boilerplate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from markitdown import MarkItDown\n",
    "import re\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".envrc\")\n",
    "\n",
    "API_KEY = os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "BASE_URL = \"https://api.deepseek.com\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "# MODEL = \"deepseek-reasoner\"\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to make calls to DeepSeek (includes json output formatting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt: str, with_json_output: bool = False) -> tuple[str | dict, str]:\n",
    "    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "    args = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [],\n",
    "    }\n",
    "\n",
    "    if with_json_output is True:\n",
    "        json_prompt = \"\"\"\n",
    "        Output your response in JSON format with the keys specified in the prompt.\n",
    "        Do not include any other text such as ```json or ```.\n",
    "        The response should be directly parseable by json.loads.\n",
    "        \"\"\".strip()\n",
    "        args[\"messages\"].append({\"role\": \"system\", \"content\": json_prompt})\n",
    "        args[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "    args[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(**args)\n",
    "\n",
    "    # if hasattr(response.choices[0].message, \"reasoning_content\"):\n",
    "    #     reasoning = response.choices[0].message.reasoning_content\n",
    "    #     final_response = response.choices[0].message.content\n",
    "\n",
    "    reasoning = \"\"\n",
    "    final_response = response.choices[0].message.content\n",
    "\n",
    "    if with_json_output is True:\n",
    "        return json.loads(final_response), reasoning\n",
    "\n",
    "    return final_response, reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create our generator. Responsible for generating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = (\n",
    "        f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    )\n",
    "    response, thoughts = llm_call(full_prompt)\n",
    "    result = re.search(r\"<RESPONSE>(.*?)</RESPONSE>\", response, re.DOTALL).group(1)\n",
    "\n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "\n",
    "    print(\"\\n*** THOUGHTS START ***\")\n",
    "    print(thoughts)\n",
    "    print(\"\\n*** THOUGHTS END ***\")\n",
    "\n",
    "    print(\"\\n*** RESULT START ***\")\n",
    "    print(result)\n",
    "    print(\"\\n*** RESULT END ***\")\n",
    "\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "\n",
    "    return thoughts, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create our evaluator, which is responsible for providing feedback on the task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response, thoughts = llm_call(full_prompt, with_json_output=True)\n",
    "    evaluation = response.get(\"evaluation\")\n",
    "    feedback = response.get(\"feedback\")\n",
    "\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "\n",
    "    print(\"\\n*** THOUGHTS START ***\")\n",
    "    print(thoughts)\n",
    "    print(\"\\n*** THOUGHTS END ***\")\n",
    "\n",
    "    print(\"\\n*** STATUS START ***\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(\"\\n*** STATUS END ***\")\n",
    "\n",
    "    print(\"\\n*** FEEDBACK START ***\")\n",
    "    print(feedback)\n",
    "    print(\"\\n*** FEEDBACK END ***\")\n",
    "\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "\n",
    "    return evaluation, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our main agentic loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(\n",
    "    task: str, evaluator_prompt: str, generator_prompt: str\n",
    ") -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "\n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    # TODO: add the reasoning content to the chain of thought\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "\n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "\n",
    "        context = \"\\n\".join(\n",
    "            [\n",
    "                \"Previous attempts:\",\n",
    "                *[f\"- {m}\" for m in memory],\n",
    "                f\"\\nFeedback: {feedback}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function to fetch papers from the arxiv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_arxiv_paper(url: str) -> str:\n",
    "    md = MarkItDown(enable_plugins=True)\n",
    "    result = md.convert(url)\n",
    "    return result.text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our prompts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "\n",
    "Evaluate the following summary. A good summary should:\n",
    "\n",
    "1. Be understandable by an undergraduate student\n",
    "2. Formatted in markdown, with proper headings and subheadings\n",
    "3. Have a title and a clear structure\n",
    "4. Have at least 500 words\n",
    "5. Grammar and spelling should be correct\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "Output your evaluation concisely in the following format:\n",
    "\n",
    "EXAMPLE JSON OUTPUT:\n",
    "{\n",
    "    \"evaluation\": \"PASS, NEEDS_IMPROVEMENT, or FAIL\",\n",
    "    \"feedback\": \"What needs improvement and why.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format:\n",
    "\n",
    "<RESPONSE>\n",
    "Content of the response\n",
    "</RESPONSE>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_agentic_summary_for(paper_url: str) -> str:\n",
    "\n",
    "    web_page_text = get_text_from_arxiv_paper(paper_url)\n",
    "\n",
    "    task = f\"\"\"\n",
    "    <user input>\n",
    "    Write a summary of the following article:\n",
    "\n",
    "    <article>\n",
    "    {web_page_text}\n",
    "    </article>\n",
    "\n",
    "    </user input>\n",
    "    \"\"\"\n",
    "\n",
    "    result, cot = loop(task, evaluator_prompt, generator_prompt)\n",
    "\n",
    "    return result, cot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We launch the agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H2' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H3' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H4' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "\n",
      "*** THOUGHTS START ***\n",
      "\n",
      "\n",
      "*** THOUGHTS END ***\n",
      "\n",
      "*** RESULT START ***\n",
      "\n",
      "MiniMax-M1 is an open-weight, large-scale hybrid-attention reasoning model powered by a Mixture-of-Experts (MoE) architecture and a lightning attention mechanism. Developed from MiniMax-Text-01, it features 456 billion parameters (45.9B activated per token) and supports a context length of 1 million tokensâ€”8x longer than DeepSeek R1. The lightning attention reduces FLOPs by 75% for 100K-token generations compared to DeepSeek R1, enhancing efficiency for long-context tasks. \n",
      "\n",
      "Key innovations include:\n",
      "1. **CISPO**: A novel RL algorithm that clips importance sampling weights instead of token updates, improving training efficiency (2x faster than DAPO).\n",
      "2. **Hybrid Attention**: Combines transformer blocks with linear attention (Transnormer blocks) for near-linear scaling in long sequences.\n",
      "3. **Training Efficiency**: Full RL training completed in 3 weeks on 512 H800 GPUs ($534,700 cost).\n",
      "\n",
      "MiniMax-M1 excels in software engineering, tool use, and long-context benchmarks, outperforming models like DeepSeek-R1 and Qwen3-235B. It is publicly released with 40K and 80K \"thinking budget\" versions, the latter showing superior performance in complex tasks. The model is optimized for real-world applications requiring extended reasoning and is available via GitHub, Hugging Face, and a commercial API.\n",
      "\n",
      "\n",
      "*** RESULT END ***\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "\n",
      "*** THOUGHTS START ***\n",
      "\n",
      "\n",
      "*** THOUGHTS END ***\n",
      "\n",
      "*** STATUS START ***\n",
      "Status: NEEDS_IMPROVEMENT\n",
      "\n",
      "*** STATUS END ***\n",
      "\n",
      "*** FEEDBACK START ***\n",
      "The summary is concise and covers key points but lacks the required 500 words, proper markdown formatting with headings and subheadings, and a clear title. It also misses some structural elements like an introduction and conclusion, which are essential for a comprehensive summary.\n",
      "\n",
      "*** FEEDBACK END ***\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "\n",
      "*** THOUGHTS START ***\n",
      "\n",
      "\n",
      "*** THOUGHTS END ***\n",
      "\n",
      "*** RESULT START ***\n",
      "\n",
      "# MiniMax-M1: A Breakthrough in Efficient Large-Scale Reasoning Models\n",
      "\n",
      "## Introduction  \n",
      "MiniMax-M1 represents a significant advancement in large reasoning models (LRMs), combining a hybrid Mixture-of-Experts (MoE) architecture with a novel **lightning attention mechanism**. Developed from MiniMax-Text-01, this open-weight model achieves unprecedented efficiency in processing long-context tasks while maintaining high performance across diverse benchmarks. With 456 billion parameters (45.9B activated per token) and support for **1 million tokens of context length**, MiniMax-M1 outperforms competitors like DeepSeek-R1 and Qwen3-235B in software engineering, tool use, and long-context understanding.\n",
      "\n",
      "---\n",
      "\n",
      "## Key Innovations  \n",
      "\n",
      "### 1. **Lightning Attention Mechanism**  \n",
      "   - Reduces FLOPs by **75%** for 100K-token generations compared to traditional attention.  \n",
      "   - Enables **near-linear scaling** for long sequences via a hybrid design:  \n",
      "     - Combines transformer blocks (softmax attention) with **Transnormer blocks** (linear attention).  \n",
      "   - Supports **8x longer context** than DeepSeek R1 (1M vs. 128K tokens).  \n",
      "\n",
      "### 2. **CISPO: A Novel RL Algorithm**  \n",
      "   - **Clips importance sampling weights** instead of token updates, stabilizing training.  \n",
      "   - Achieves **2x faster training** than DAPO (Yu et al., 2025) by preserving gradient contributions from all tokens.  \n",
      "   - Empirical validation shows superior performance on mathematical reasoning benchmarks (e.g., AIME 2024).  \n",
      "\n",
      "### 3. **Training Efficiency**  \n",
      "   - Full RL training completed in **3 weeks** on 512 H800 GPUs ($534,700 cost).  \n",
      "   - Techniques like **FP32 precision for LM heads** and **early truncation heuristics** mitigate instability during scaling.  \n",
      "\n",
      "---\n",
      "\n",
      "## Performance Highlights  \n",
      "\n",
      "### Benchmark Results  \n",
      "MiniMax-M1 excels in:  \n",
      "- **Software Engineering (SWE-bench)**: 56% accuracy (vs. 57.6% for DeepSeek-R1-0528).  \n",
      "- **Long Context (OpenAI-MRCR)**: Outperforms OpenAI o3 and Claude 4 Opus, trailing only Gemini 2.5 Pro.  \n",
      "- **Agentic Tool Use (TAU-bench)**: Surpasses all open-weight models and Gemini 2.5 Pro.  \n",
      "\n",
      "| Model            | AIME 2024 | LiveCodeBench | SWE-bench | TAU-bench |  \n",
      "|------------------|-----------|---------------|-----------|-----------|  \n",
      "| MiniMax-M1-80k   | 86.0%     | 70.1%         | 56.0%     | 73.4%     |  \n",
      "| DeepSeek-R1-0528 | 88.0%     | 75.5%         | 57.6%     | 67.2%     |  \n",
      "\n",
      "### Extended Thinking  \n",
      "- **80K-token generation** version (MiniMax-M1-80k) shows consistent gains over the 40K variant, validating the benefits of scaled test-time compute.  \n",
      "\n",
      "---\n",
      "\n",
      "## Architectural and Training Insights  \n",
      "\n",
      "### Hybrid Attention Design  \n",
      "- Alternates **7 Transnormer blocks** (linear attention) with **1 transformer block** (softmax attention) per layer.  \n",
      "- Theoretical FLOPs scale near-linearly with sequence length (Figure 1).  \n",
      "\n",
      "### RL Training Pipeline  \n",
      "1. **Continual Pretraining**: 7.5T tokens of reasoning-intensive data (70% STEM/code).  \n",
      "2. **Supervised Fine-Tuning (SFT)**: Injects chain-of-thought (CoT) patterns for RL readiness.  \n",
      "3. **RL with CISPO**: Curriculum includes verifiable (math, coding) and generative tasks (QA, creative writing).  \n",
      "\n",
      "### Challenges Addressed  \n",
      "- **Precision Mismatch**: FP32 precision for LM heads resolved training-inference probability gaps.  \n",
      "- **Length Bias**: Online monitoring and reward shaping prevent reward hacking via verbose outputs.  \n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion and Future Directions  \n",
      "MiniMax-M1 sets a new standard for **efficient, scalable reasoning models**, with open-weight availability via [GitHub](https://github.com/MiniMax-AI/MiniMax-M1) and commercial API. Future work aims to:  \n",
      "- Expand applications in **real-world workflows** (e.g., automated software engineering).  \n",
      "- Further optimize **long-context agentic interactions**.  \n",
      "\n",
      "By combining architectural efficiency with algorithmic innovations like CISPO, MiniMax-M1 paves the way for next-generation language model agents.\n",
      "\n",
      "\n",
      "*** RESULT END ***\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "\n",
      "*** THOUGHTS START ***\n",
      "\n",
      "\n",
      "*** THOUGHTS END ***\n",
      "\n",
      "*** STATUS START ***\n",
      "Status: PASS\n",
      "\n",
      "*** STATUS END ***\n",
      "\n",
      "*** FEEDBACK START ***\n",
      "The summary meets all the specified criteria: it is understandable by an undergraduate student, formatted in markdown with proper headings and subheadings, has a title and clear structure, exceeds 500 words, and maintains correct grammar and spelling.\n",
      "\n",
      "*** FEEDBACK END ***\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result, cot = generate_agentic_summary_for(\"https://arxiv.org/pdf/2506.13585\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# MiniMax-M1: A Breakthrough in Efficient Large-Scale Reasoning Models\n",
       "\n",
       "## Introduction  \n",
       "MiniMax-M1 represents a significant advancement in large reasoning models (LRMs), combining a hybrid Mixture-of-Experts (MoE) architecture with a novel **lightning attention mechanism**. Developed from MiniMax-Text-01, this open-weight model achieves unprecedented efficiency in processing long-context tasks while maintaining high performance across diverse benchmarks. With 456 billion parameters (45.9B activated per token) and support for **1 million tokens of context length**, MiniMax-M1 outperforms competitors like DeepSeek-R1 and Qwen3-235B in software engineering, tool use, and long-context understanding.\n",
       "\n",
       "---\n",
       "\n",
       "## Key Innovations  \n",
       "\n",
       "### 1. **Lightning Attention Mechanism**  \n",
       "   - Reduces FLOPs by **75%** for 100K-token generations compared to traditional attention.  \n",
       "   - Enables **near-linear scaling** for long sequences via a hybrid design:  \n",
       "     - Combines transformer blocks (softmax attention) with **Transnormer blocks** (linear attention).  \n",
       "   - Supports **8x longer context** than DeepSeek R1 (1M vs. 128K tokens).  \n",
       "\n",
       "### 2. **CISPO: A Novel RL Algorithm**  \n",
       "   - **Clips importance sampling weights** instead of token updates, stabilizing training.  \n",
       "   - Achieves **2x faster training** than DAPO (Yu et al., 2025) by preserving gradient contributions from all tokens.  \n",
       "   - Empirical validation shows superior performance on mathematical reasoning benchmarks (e.g., AIME 2024).  \n",
       "\n",
       "### 3. **Training Efficiency**  \n",
       "   - Full RL training completed in **3 weeks** on 512 H800 GPUs ($534,700 cost).  \n",
       "   - Techniques like **FP32 precision for LM heads** and **early truncation heuristics** mitigate instability during scaling.  \n",
       "\n",
       "---\n",
       "\n",
       "## Performance Highlights  \n",
       "\n",
       "### Benchmark Results  \n",
       "MiniMax-M1 excels in:  \n",
       "- **Software Engineering (SWE-bench)**: 56% accuracy (vs. 57.6% for DeepSeek-R1-0528).  \n",
       "- **Long Context (OpenAI-MRCR)**: Outperforms OpenAI o3 and Claude 4 Opus, trailing only Gemini 2.5 Pro.  \n",
       "- **Agentic Tool Use (TAU-bench)**: Surpasses all open-weight models and Gemini 2.5 Pro.  \n",
       "\n",
       "| Model            | AIME 2024 | LiveCodeBench | SWE-bench | TAU-bench |  \n",
       "|------------------|-----------|---------------|-----------|-----------|  \n",
       "| MiniMax-M1-80k   | 86.0%     | 70.1%         | 56.0%     | 73.4%     |  \n",
       "| DeepSeek-R1-0528 | 88.0%     | 75.5%         | 57.6%     | 67.2%     |  \n",
       "\n",
       "### Extended Thinking  \n",
       "- **80K-token generation** version (MiniMax-M1-80k) shows consistent gains over the 40K variant, validating the benefits of scaled test-time compute.  \n",
       "\n",
       "---\n",
       "\n",
       "## Architectural and Training Insights  \n",
       "\n",
       "### Hybrid Attention Design  \n",
       "- Alternates **7 Transnormer blocks** (linear attention) with **1 transformer block** (softmax attention) per layer.  \n",
       "- Theoretical FLOPs scale near-linearly with sequence length (Figure 1).  \n",
       "\n",
       "### RL Training Pipeline  \n",
       "1. **Continual Pretraining**: 7.5T tokens of reasoning-intensive data (70% STEM/code).  \n",
       "2. **Supervised Fine-Tuning (SFT)**: Injects chain-of-thought (CoT) patterns for RL readiness.  \n",
       "3. **RL with CISPO**: Curriculum includes verifiable (math, coding) and generative tasks (QA, creative writing).  \n",
       "\n",
       "### Challenges Addressed  \n",
       "- **Precision Mismatch**: FP32 precision for LM heads resolved training-inference probability gaps.  \n",
       "- **Length Bias**: Online monitoring and reward shaping prevent reward hacking via verbose outputs.  \n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion and Future Directions  \n",
       "MiniMax-M1 sets a new standard for **efficient, scalable reasoning models**, with open-weight availability via [GitHub](https://github.com/MiniMax-AI/MiniMax-M1) and commercial API. Future work aims to:  \n",
       "- Expand applications in **real-world workflows** (e.g., automated software engineering).  \n",
       "- Further optimize **long-context agentic interactions**.  \n",
       "\n",
       "By combining architectural efficiency with algorithmic innovations like CISPO, MiniMax-M1 paves the way for next-generation language model agents.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise**\n",
    "- We are using the `deepseek-chat` model, can you change this notebook to use the reasoner? \n",
    "- How do we integrate the reasoning process into our evaluator optimizer? (hint: You need to change the code in two places!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now it's time to move to function calling agents! No more workflows :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
